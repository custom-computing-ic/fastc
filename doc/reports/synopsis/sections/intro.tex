Traditional computing architectures process instructions sequentially
fetching them from computer memory, executing them and recording their
results. This provides a convenient programming model but limits
performance and energy efficiency. Transition to a dataflow
architecture, where data are instead streamed through a static
instruction graph can improve performance and energy efficiency by
orders of magnitude, drastically reducing computation time in vital
areas such as weather forecasting, oil and gas exploration and
financial risk assessment. One crucial obstacle that remains in the
way of dataflow computing is the reduced developer productivity and
this is what we aim to overcome in this project.

\section{Introduction}

Existing work shows that dataflow architectures can achieve
significant performance gains compared to multi-core processors when
implementing high throughput, highly parallel applications that
operate on large, uniform data sets \cite{Flynn:Pell:Mencer:2012,
  Mencer:2012}.  However, due to poor developer productivity, the
dataflow paradigm is not widely adopted and imperative languages are
significantly more popular \cite{Tiobe:2012}.

One area where we observe significant improvements through the
adoption of the dataflow paradigm is High-Performance
Computing. High-performance applications require a significant amount
of computational resources and the quest for exascale computing
requires that we develop ever faster and more energy efficient
architectures. By emulating dataflow machines on Field Programmable
Gate Arrays -- programmable logic chips -- orders of magnitude
increase in performance and energy efficiency can be achieved.  For
example, an implementation of Reverse Time Migration -- an advanced
seismic imaging application used by Chevron for oil and gas
exploration -- has been shown to be 103 times faster and 145 times
more energy efficient than an optimised implementation running on a
multi-core microprocessor
\cite{Xinyu:Qiwei:Luk:Qiang:Pell:2012}. Other important applications
of dataflow computing include the recent deployment of a dataflow
cluster at J.P. Morgan which reduced the computation time for complex
risk assessment scenarios from hours to minutes.

In this project we propose a novel design flow for High-Performance
Computing (HPC) applications, meant to provide:
\begin{itemize}
\item \textbf{more efficient HPC} -- by using dataflow engines based on
  \emph{Field Programmable Gate Arrays} that can achieve orders of
  magnitude increase in performance and decrease in energy consumption
\item \textbf{more productive HPC} -- by using \emph{Aspect Oriented
  Programming} to decouple application optimisation from application
  development thus promoting portability and improving maintainability
\end{itemize}

\begin{comment}
We provide  brief overview of FPGA based dataflow computing and
Aspect-Oriented Programming in \Cref{sec:background}. We then explain
how these elements can be combined as part of a novel design flow to
produce efficient applications with increased productivity in
\Cref{sec:design-flow}. We provide a brief description of our
experimental implementation in \Cref{sec:implementation} and analyse
the preliminary results of applying our design flow to important
industrial applications in \Cref{sec:evaluation}.
\end{comment}

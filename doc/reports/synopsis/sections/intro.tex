Traditional computing architectures process instructions sequentially,
fetching them from computer memory, executing them and recording their
results. This provides a convenient programming model but limits
performance and energy efficiency. Transition to a dataflow
architecture, where data are instead streamed through a static
instruction graph can improve performance and energy efficiency by
orders of magnitude, drastically reducing computation time in vital
areas such as weather forecasting, oil and gas exploration and
financial risk assessment. One key challenge is to improve the quality
of dataflow programs and to enhance the productivity of dataflow
programmers; it is what we aim to address in this project.

\section{Introduction}

Existing work shows that dataflow architectures can achieve
significant performance gains compared to multi-core processors when
implementing high throughput, highly parallel applications that
operate on large, uniform data sets \cite{Flynn:Pell:Mencer:2012,
  Mencer:2012}.  However, due to poor developer productivity, the
dataflow paradigm is not widely adopted and imperative languages are
significantly more popular \cite{Tiobe:2012}.

High-performance applications could benefit greatly from the adoption
of dataflow computing since they require a significant amount of
computational resources. By emulating dataflow machines on Field
Programmable Gate Arrays (FPGAs) -- programmable hardware chips --
orders of magnitude increase in performance and energy efficiency can
be achieved.  For example, an implementation of Reverse Time Migration
-- an advanced seismic imaging application used by Chevron for oil and
gas exploration -- has been shown to be 103 times faster and 145 times
more energy efficient than an optimised implementation running on a
multi-core microprocessor
\cite{Xinyu:Qiwei:Luk:Qiang:Pell:2012}. Other important applications
of dataflow computing include the recent deployment of a dataflow
cluster at J.P. Morgan which reduced the computation time for complex
risk assessment scenarios from hours to minutes.

In this project we propose a novel design-flow for High-Performance
Computing (HPC) applications to support:
\begin{itemize}
\item \textbf{more efficient HPC} -- by using FPGA based dataflow
  engines to achieve drastic increase in performance and energy
  efficiency;
\item \textbf{more productive HPC} -- by using \emph{Aspect Oriented
    Programming} to decouple application optimisation from application
  development improving portability and maintainability.
\end{itemize}

\begin{comment}
We provide  brief overview of FPGA based dataflow computing and
Aspect-Oriented Programming in \Cref{sec:background}. We then explain
how these elements can be combined as part of a novel design flow to
produce efficient applications with increased productivity in
\Cref{sec:design-flow}. We provide a brief description of our
experimental implementation in \Cref{sec:implementation} and analyse
the preliminary results of applying our design flow to important
industrial applications in \Cref{sec:evaluation}.
\end{comment}
